\documentclass{article} % For LaTeX2e
% \usepackage{nips13submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{textcomp}  % \textcelsius
% \graphicspath{{./fig/}}
% \usepackage{tikz}

\title{Homework 1}
\author{Zhong Yun 2016K8009915009}
\usepackage{geometry}
\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newtheorem{theorem}{Theorem}
\newtheorem{proof}{Proof}

\begin{document}


\maketitle
%
%\begin{abstract}
%empty
%\end{abstract}


% \input{problem-and-notation.tex}
%\input{aaa}

\begin{theorem}
Information Gain $Gain(X,Y)\ge 0$
\end{theorem}
\begin{proof}
\emph{The definition of Information Gain is as follows:}
\[Gain(X,Y) = H(X)-H(X|Y)\qquad (1)\]

\emph{According to $Jensen\ Inequality$: if function f(x) is convex, g(x) is an arbitrary function about f(x), p(x)$\ge0$, then} 
\[{\cfrac{\int_a^b f(g(x))p(x)dx}{\int_a^b p(x)dx}\ge f\left(\cfrac{\int_a^bg(x)p(x)dx}{\int_a^bp(x)dx}\right)}\]

\emph{Meanwhile, according to the definition of $Kullback-Leibler\ divergence$,}
\[D(P\|Q) = \sum_{x\in X}p(x)log\left(\frac{p(x)}{q(x)}\right) = \int_{x\in X}p(x)log\left(\frac{p(x)}{q(x)}\right)dx\]


\emph{Among them,$p(x)$ indicates the probability of $x$, $q(x)$ indicates the probability density of $x$.\\Then, we derive (1)}

\[\begin{aligned}
Gain(X,Y) ={}& H(X)-H(X|Y){} \\
& = -\sum_{x\in X}p(x)log(p(x))+\sum_{y\in Y}p(y)\sum_{x\in X}p(x|y)log(p(x|y))\\
& = \sum_{x\in X}\sum_{y\in Y}p(x,y)(\log(p(x))-log(p(y))+log(p(x,y)))\\
& = \sum_{x\in X}\sum_{y\in Y}p(x,y)log\left(\cfrac{p(x,y)}{p(x)p(y)}\right)\\
& = D(P(X,Y)\|P(X)P(Y))
\end{aligned}\]


\emph{Then we have}
\[Gain(X,Y) = D(P(X,Y)\|P(X)P(Y))\]


\emph{Therefore, we only need to prove $Kullback-Leibler\ divergence\ formula\ge 0$. The proof is as follows:}
\[\begin{aligned}
D(P\|Q) ={}& \int_{x\in X}p(x)log\left(\frac{p(x)}{q(x)}\right){} \\
& =\int_{x\in X}-log\left(\frac{q(x)}{p(x)}\right)p(x)dx
\end{aligned}\]


\emph{Here, we regard $-log(x)$ as f(x), regard $\frac{q(x)}{p(x)}$ as g(x), and there is $\int_{x\in X}p(x)dx = 1$, according to $Jensen\ Inquality\ Formula$,}
\[\begin{aligned}
\int_{x\in X}-log\left(\frac{q(x)}{p(x)}\right)p(x)dx \ge {}&-log\left(\int_{x\in X}\frac{q(x)}{p(x)}p(x)dx\right){} \\
& = -log\left(\int_{x\in X}q(x)dx\right)\\
& = -log(1) \\
& = 0
\end{aligned}\]

\emph{Then, we have $D(P\|Q) \ge 0$, therefore, $Gain(X,Y) \ge 0$, Therom 1 is proved.}
\end{proof}
% \input{our-approach.tex}
%
% \section{Evaluation}
% [outline]which model to compare? How to convince readers that the problem is difficult and our model is good.
%
% \section{Speed Up}
% [outline]\\
% 1. Basic Model: node by node\\
% 2. Speed up model: layer by layer
%
% \input{related-work.tex}
%
% \subsubsection*{References}

\end{document}
