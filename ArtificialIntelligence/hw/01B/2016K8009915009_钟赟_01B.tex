\documentclass{article} % For LaTeX2e
% \usepackage{nips13submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{textcomp}  % \textcelsius
% \graphicspath{{./fig/}}
% \usepackage{tikz}

\title{Homework 1}
\author{Zhong Yun 2016K8009915009}
\usepackage{geometry}
\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newtheorem{theorem}{Problem}
\newtheorem{proof}{Answer}

\begin{document}


\maketitle
%
%\begin{abstract}
%empty
%\end{abstract}


% \input{problem-and-notation.tex}
%\input{aaa}
\begin{theorem}
Construct a data, each conditional data has its own value. The constructed data should prove that the decision tree calculated with greedy algorithm is not an optimal decision tree.
\end{theorem}
\begin{proof}
\emph{Example:Suppose there are three key factors that determine if a basketball game wins or loses. Data Set is as follows:\\}

\begin{tabular}{|c|c|c|c|c|}
 \hline
\textbf{No.} & \textbf{Win or Lose} & \textbf{Time} & \textbf{Be home or away} & \textbf{Weather}\\
 \hline
1 & win & morning & home & sunny \\
 \hline
2 & win & morning & home & rain \\
 \hline
3 & win & morning & away & rain \\
 \hline
4 & win & morning & home & rain \\
 \hline
5 & lose & morning & away & sunny \\
 \hline
6 & lose & night & home & sunny \\
 \hline
\end{tabular}
\\\\

\emph{First, let's create a decision tree using Greedy Algorithm:\\ Conditional Entropy:(Here we use Shannon Information Entropy to compute Information Entropy)}

\[\begin{aligned}
H(Win\ or\ Lose|Time) ={}& P(morning)H(Win\ or\ Lose|morning) + P(night)H(Win\ or\ Lose|night){}\\
& = P(morning)\left(P(win|morning)log_2 \frac{1}{P(win|morning)}+P(lose|morning)log_2 \frac{1}{P(lose|morning)}\right) + P(night)\left(P(win|night)log_2 \frac{1}{P(win|night)}+P(lose|night)log_2 \frac{1}{P(lose|night)}\right)\\
& = 0.918
\end{aligned}
\]
\emph{Similarly, we have that}
\begin{align*}
H(Win\ or\ Lose|Be\ home\ or\ away) &= 0.874 \\
H(Win\ or\ Lose|Weather) &= 0.459\\
&\cdots
\end{align*}

\emph{Finally, we have a decision tree(see tree [a]):\\}
\includegraphics[width = .6\textwidth]{tree_a.jpg}\\
\emph{Test the data set given,the accuracy of decision tree [a] is $\frac{5}{6}$.(The sixth data do not comform to decisicion tree [a] )\\\\
However, we can create a better decision tree b for the data set(see tree [b]):\\}
\includegraphics[width = .6\textwidth]{tree_b.jpg}\\
\emph{The accuracy of tree [b] is $\frac{6}{6} = 1 \geq \frac{5}{6}$.\\
It is proved that decision tree calculated with greedy algorithm may not be an optimal decision tree.}
\end{proof}







% \input{our-approach.tex}
%
% \section{Evaluation}
% [outline]which model to compare? How to convince readers that the problem is difficult and our model is good.
%
% \section{Speed Up}
% [outline]\\
% 1. Basic Model: node by node\\
% 2. Speed up model: layer by layer
%
% \input{related-work.tex}
%
% \subsubsection*{References}

\end{document}















